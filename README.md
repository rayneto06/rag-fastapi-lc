# RAG FastAPI (LangChain + LangSmith + Clean Architecture)

Este projeto demonstra **Retrieval-Augmented Generation (RAG)** com:
- **LangChain** (LCEL, loaders, splitters, retrievers, embeddings)
- **Chroma** como VectorStore persistente local
- **LangSmith** para tracing/observabilidade (opcional via env)
- **FastAPI** como camada web
- **Clean Architecture** (dom√≠nio e casos de uso independentes de frameworks)

---

## üß± Arquitetura (pastas)

```
app/                       # settings, container e app factory
domain/                    # contratos (protocols) e modelos de dom√≠nio
use_cases/                 # aplica√ß√£o (ingest, query_rag, etc.)
infrastructure/            # adapters (LangChain, Chroma, loaders, observability)
interface_adapters/        # web (FastAPI routers)
tests/                     # testes unit√°rios e E2E m√≠nimos
```

Fluxo (simplificado): **/v1/documents ‚Üí ingest (loader + splitter + Chroma)** ‚Üí **/v1/rag/query ‚Üí retriever (MMR/k) ‚Üí (opcional) gera√ß√£o determin√≠stica**.

---

## ‚úÖ Step 1 ‚Äî Echo + LangSmith

- Rota de sanidade: `POST /v1/echo` usando LCEL (`RunnableLambda`) que apenas ecoa a pergunta.
- LangSmith habilitado via `.env` (opcional). Sem chave, o app n√£o quebra.

## üì• Step 2 ‚Äî Ingest√£o com Chroma (LangChain)

- Loader de PDF: `PyMuPDFLoader`.
- Chunking: `RecursiveCharacterTextSplitter`.
- Persist√™ncia: `Chroma` com `chromadb.PersistentClient`.
- Endpoint: `POST /v1/documents` (form-data `file` PDF).
- Teste: `tests/test_ingest.py` (gera PDF sint√©tico e valida ingest).

## üîé Step 3 ‚Äî Retrieval-first RAG + ‚ÄúG‚Äù opcional (determin√≠stico)

- Provider de **embeddings** (default: `FakeEmbeddings`) injetado no `Chroma`.
- `as_retriever(search_type="mmr", k=5)` para consulta.
- Caso de uso `QueryRAGUseCase` retorna `hits` e, se `generate=true`, formata resposta determin√≠stica a partir do contexto.
- Endpoint: `POST /v1/rag/query`.
- Teste: `tests/test_rag_retrieval.py`.

> **Step 4 (pr√≥ximo)**: plugar um `LLMProvider` real (Ollama/OpenAI) mantendo a interface no dom√≠nio e um teste de integra√ß√£o *skippable* por env.

---

## üõ†Ô∏è Setup

```bash
python -m venv .venv && source .venv/bin/activate     # Windows: .venv\Scripts\activate
pip install -e .[dev]
cp .env.example .env
uvicorn app.main:app --reload
# Docs: http://127.0.0.1:8000/docs
```

### Vari√°veis de ambiente (.env)

```dotenv
# Observabilidade (LangSmith) ‚Äî opcionais
LANGCHAIN_TRACING_V2=false
LANGCHAIN_API_KEY=
LANGSMITH_PROJECT=rag-fastapi-lc

# Armazenamento / Chroma
VECTOR_STORE_PROVIDER=chroma
CHROMA_DIR=.chroma
RAW_DIR=data/raw
CHROMA_COLLECTION=documents

# Embeddings
EMBEDDINGS_PROVIDER=fake     # fake | ollama | openai
EMBEDDINGS_MODEL=fake        # ex.: nomic-embed-text (Ollama)
# OLLAMA_BASE_URL=http://localhost:11434
# OPENAI_API_KEY=
```

---

## üß™ Testes

```bash
pytest -q
# ou arquivos espec√≠ficos:
pytest -q tests/test_echo.py
pytest -q tests/test_ingest.py
pytest -q tests/test_rag_retrieval.py
```

---

## üß≠ Endpoints

### `POST /v1/echo`
```json
{ "question": "ping" } -> { "answer": "echo: ping" }
```

### `POST /v1/documents`
- Form-data: `file` (PDF)
- Resposta 201: `{ "filename": "...", "uploaded_at": "...", "num_docs": 1, "num_chunks": 12, "collection": "documents" }`

### `POST /v1/rag/query`
- Payload: `{ "question": "‚Ä¶", "generate": false, "k": 5, "search_type": "mmr" }`
- Resposta: `{ "answer": null|"...", "hits": [{ "content": "...", "metadata": {...} }, ...] }`

---

## üß≠ Roadmap sugerido (pequenos & test√°veis)
1. **LLMProvider real (Ollama/OpenAI)** com adapter LCEL e teste de integra√ß√£o skip√°vel.
2. **Cita√ß√µes** nos resultados (ID/URI do chunk) e ordena√ß√£o por score.
3. **Streaming** da gera√ß√£o via Server-Sent Events (SSE).
4. **Avalia√ß√µes no LangSmith** (datasets / runs com metadados de chunking e k).

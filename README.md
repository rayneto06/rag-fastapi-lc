# ğŸ§  RAG FastAPI (LangChain + LangSmith + Clean Architecture)

A fully modular **Retrieval-Augmented Generation (RAG)** API built with **FastAPI**, following **Clean Architecture**, and powered by **LangChain**, **LangSmith**, and local or cloud LLMs (OpenAI / Ollama).  
Designed as a didactic, production-quality example for study and portfolio use.

---

## ğŸš€ Overview

This project demonstrates an end-to-end RAG pipeline:

```
Upload PDF â†’ Split into Chunks â†’ Embed â†’ Store in Chroma â†’ Query â†’ Retrieve Context â†’ Generate Answer
```

### Core Capabilities

| Feature | Description |
|----------|-------------|
| ğŸ“ Document Upload | Upload PDFs and persist embeddings in Chroma |
| ğŸ” Context Retrieval | Query Chroma using similarity / MMR search |
| ğŸ§  Generation | Deterministic or real generation using OpenAI / Ollama |
| âš™ï¸ Architecture | Clean Architecture (Domain, Use Cases, Infrastructure, Web) |
| ğŸ§ª Testing | Full unit + integration suite with fake and real LLM providers |
| ğŸ“Š Observability | Optional LangSmith tracing and metrics |

---

## ğŸ§© Project Structure

```
rag-fastapi-lc/
â”œâ”€â”€ app/                        # FastAPI entrypoint and settings
â”œâ”€â”€ domain/                     # Core interfaces (LLMProvider, etc.)
â”œâ”€â”€ use_cases/                  # Business logic (Ingest, QueryRAG)
â”œâ”€â”€ infrastructure/             # Adapters (Chroma, LangChain LLMs, Embeddings)
â”œâ”€â”€ interface_adapters/         # FastAPI routers (v1/documents, v1/rag)
â”œâ”€â”€ tests/                      # Unit & integration tests
â”œâ”€â”€ .env.example                # Template environment variables
â”œâ”€â”€ pyproject.toml              # Dependencies and dev tools
â””â”€â”€ README.md                   # You are here
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/rayneto06/rag-fastapi-lc.git
cd rag-fastapi-lc
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -e .[dev]
cp .env.example .env
```

---

## ğŸ”§ Configuration (.env)

Edit your `.env` file to choose providers and settings.

### Example for local Ollama:

```dotenv
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1
EMBEDDINGS_PROVIDER=ollama
EMBEDDINGS_MODEL=nomic-embed-text
CHROMA_COLLECTION=documents_ollama
LANGCHAIN_TRACING_V2=false
```

### Example for OpenAI:

```dotenv
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
EMBEDDINGS_PROVIDER=openai
OPENAI_API_KEY=sk-...
```

### Example for fake (deterministic testing):

```dotenv
LLM_PROVIDER=fake
EMBEDDINGS_PROVIDER=fake
```

---

## ğŸ§  Running Locally

### Start the API

```bash
uvicorn app.main:app --reload
```

Docs available at: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

### Start Ollama (if local)

```bash
ollama serve
ollama pull llama3.1
ollama pull nomic-embed-text
```

---

## ğŸ“¬ API Endpoints

| Method | Endpoint | Description |
|---------|-----------|-------------|
| `POST` | `/v1/echo` | Simple echo test |
| `POST` | `/v1/documents` | Upload and ingest a PDF into Chroma |
| `GET`  | `/v1/documents` | Get collection stats |
| `POST` | `/v1/rag/query` | Perform retrieval and (optional) generation |

### Example Query (JSON)

```json
{
  "question": "Resuma o documento",
  "generate": true,
  "k": 4,
  "search_type": "mmr"
}
```

---

## ğŸ§ª Testing

### Run all tests

```bash
pytest -q
```

### Run deterministic (fake providers only)

```bash
pytest -q -m "not integration"
```

### Run real integration tests (OpenAI or Ollama)

```bash
pytest -q -m integration
```

---

## ğŸ§± Clean Architecture Principles

- **Domain**: pure business rules (interfaces only)  
- **Use Cases**: application logic (no external dependencies)  
- **Infrastructure**: adapters for LangChain, Chroma, LLMs, embeddings  
- **Interface Adapters**: FastAPI controllers and DTOs  

Each layer depends **only inward**, enabling testing, replacement, and extension without coupling.

---

## ğŸ“Š LangSmith Tracing (optional)

Enable full tracing and experiment logging:

```dotenv
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=sk-...
LANGSMITH_PROJECT=rag-fastapi-lc
```

View runs at [https://smith.langchain.com](https://smith.langchain.com).

---

## ğŸ§° Makefile (optional)

If you use `make`, create a `Makefile` with shortcuts like:

```makefile
serve:
	uvicorn app.main:app --reload

ollama:
	ollama serve

test:
	pytest -q -m "not integration"

test-int:
	pytest -q -m integration
```

---

## ğŸ§­ Next Steps

- Add `GET /v1/rag/summarize` (full-document summaries)
- Include citation metadata (`source`, `page`, `chunk_id`) in query hits
- Add Dockerfile and CI (GitHub Actions)

---

Â© 2025 Raymundo Neto â€” Educational RAG Project
